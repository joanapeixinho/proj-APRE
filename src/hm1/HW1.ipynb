{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework I\n",
    "\n",
    "Joana Peixinho (ist1103335) & Miguel Fernandes (ist1103573)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Pen and Paper [11v]\n",
    "\n",
    "Consider the partially learnt decision tree from the dataset ùê∑. ùê∑ is described by four input variables ‚Äì\n",
    "one numeric with values in [0,1] and 3 categorical ‚Äì and a target variable with three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Decision Tree](./decision_tree.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [5v] Complete the given decision tree using Information gain with Shannon entropy (log2).\n",
    "Consider that: i) a minimum of 4 observations is required to split an internal node, and \n",
    "ii) decisions by ascending alphabetic order should be placed in case of ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) [2.5v] Draw the training confusion matrix for the learnt decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|        | True A | True B | True C |\n",
    "|--------|--------|--------|--------|\n",
    "| Pred A |   0    |   0    |   0    |\n",
    "| Pred B |   0    |   0    |   0    |\n",
    "| Pred C |   0    |   0    |   0    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [1.5v] Identify which class has the lowest training F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) [1v] Considering y2 to be ordinal, assess if y1 and y2 are correlated using the Spearman coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "# (ranks) of y1 and y2\n",
    "rank_y1 = [3, 2, 1, 5, 4, 10, 12, 11, 7, 9, 6, 8]\n",
    "rank_y2 = [8, 11, 3.5, 3.5, 3.5, 11, 3.5, 11, 8, 3.5, 8, 3.5]\n",
    "\n",
    "# Spearman's correlation coefficient\n",
    "rho, p_value = scipy.stats.spearmanr(rank_y1, rank_y2)\n",
    "\n",
    "print(\"Spearman's correlation coefficient:\", rho)\n",
    "print(\"P value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) [1v] Draw the class-conditional relative histograms of y1 using 5 equally spaced bins in [0,1].\n",
    "Challenge: find the root split using the discriminant rules from these empirical distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data for y1\n",
    "y1 = np.array([0.24, 0.06, 0.04, 0.36, 0.32, 0.68, 0.9, 0.76, 0.46, 0.62, 0.44, 0.52])\n",
    "num_bins = 5  # Number of bins (intervals)\n",
    "\n",
    "# Create the histogram of relative frequency\n",
    "plt.hist(y1, bins=num_bins, range=(0, 1), density=True, alpha=0.75, edgecolor='k')\n",
    "\n",
    "plt.xlabel('Values of y1')\n",
    "plt.ylabel('Relative Frequency')\n",
    "plt.title('Histogram of Relative Frequency of y1')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming [9v]\n",
    "\n",
    "**Considering the `column_diagnosis.arff` dataset available at the homework tab:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.io.arff import loadarff\n",
    "\n",
    "# Reading the ARFF file\n",
    "data = loadarff('column_diagnosis.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "df['class'] = df['class'].str.decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [1.5v]\n",
    "\n",
    "**Apply f_classif from sklearn to assess the discriminative power of the input variables.\n",
    "Identify the input variable with the highest and lowest discriminative power. \n",
    "Plot the class-conditional probability density functions of these two input variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import sklearn\n",
    "except ImportError:\n",
    "    !pip install scikit-learn\n",
    "    import sklearn\n",
    "\n",
    "from sklearn.feature_selection import f_classif\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "X = df.drop('class', axis=1)\n",
    "y = df['class']\n",
    "\n",
    "fimportance = f_classif(X, y)\n",
    "highest_score = fimportance[0].argmax()\n",
    "highest_feature = X.columns.values[highest_score]\n",
    "lowest_score = fimportance[0].argmin()\n",
    "lowest_feature = X.columns.values[lowest_score]\n",
    "\n",
    "print('The variable with the highest F-score is', highest_feature, \\\n",
    "      'with a score of', fimportance[0][highest_score])\n",
    "print('The variable with the lowest F-score is', lowest_feature,\\\n",
    "       'with a score of', fimportance[0][lowest_score])\n",
    "\n",
    "\n",
    "top_2_features = [highest_feature, lowest_feature]\n",
    "\n",
    "plot_df = df[top_2_features + ['class']]\n",
    "\n",
    "print(plot_df)\n",
    "\n",
    "class_hernia_data = plot_df[plot_df['class'] == 'Hernia']\n",
    "print(class_hernia_data)\n",
    "class_normal_data = plot_df[plot_df['class'] == 'Normal']\n",
    "print(class_normal_data)\n",
    "class_Spondylolisthesis_data = plot_df[plot_df['class'] == 'Spondylolisthesis']\n",
    "\n",
    "for feature in top_2_features:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.kdeplot(class_normal_data[feature], label='Normal', fill=True)\n",
    "    sns.kdeplot(class_hernia_data[feature], label='Hernia', fill=True)\n",
    "    sns.kdeplot(class_Spondylolisthesis_data[feature], label='Spondylolisthesis', fill=True)\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'Class-Conditional Probability Density for {feature}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) [4v]\n",
    "\n",
    "**Using a stratified 70-30 training-testing split with a fixed seed (random_state=0), assess in a\n",
    "single plot both the training and testing accuracies of a decision tree with depth limits in\n",
    "{1,2,3,4,5,6,8,10} and the remaining parameters as default.\n",
    "[optional] Note that split thresholding of numeric variables in decision trees is non-deterministic\n",
    "in sklearn, hence you may opt to average the results using 10 runs per parameterization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.io import arff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = arff.loadarff('column_diagnosis.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['class'] = le.fit_transform(df['class'])\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
    "\n",
    "depth_limits = [1, 2, 3, 4, 5, 6, 8, 10]\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for depth_limit in depth_limits:\n",
    "    train_acc = 0\n",
    "    test_acc = 0\n",
    "    num_runs = 10\n",
    "\n",
    "    for _ in range(num_runs):\n",
    "        clf = DecisionTreeClassifier(max_depth=depth_limit, random_state=0)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        train_acc += clf.score(X_train, y_train)\n",
    "        test_acc += clf.score(X_test, y_test)\n",
    "\n",
    "    train_accuracies.append(train_acc / num_runs)\n",
    "    test_accuracies.append(test_acc / num_runs)\n",
    "\n",
    "    \n",
    "plt.plot(depth_limits, train_accuracies, label='Training Accuracy')\n",
    "plt.plot(depth_limits, test_accuracies, label='Testing Accuracy')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Decision Tree Accuracy vs. Max Depth')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [1.5v]\n",
    "\n",
    "**Comment on the results, including the generalization capacity across settings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) [2v]\n",
    "\n",
    "**To deploy the predictor, a healthcare team opted to learn a single decision tree\n",
    "(random_state=0) using all available data as training data, and further ensuring that each leaf has\n",
    "a minimum of 20 individuals in order to avoid overfitting risks.**\n",
    "\n",
    "    i. Plot the decision tree.\n",
    "    ii. Characterize a hernia condition by identifying the hernia-conditional associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
