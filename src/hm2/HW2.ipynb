{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework I\n",
    "\n",
    "Joana Peixinho (ist1103335) & Miguel Fernandes (ist1103573)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Pen and Paper [13v]\n",
    "\n",
    "Consider the following dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Decision Tree](./decision_tree.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Consider ğ±1â€“ğ±7 to be training observations, ğ±8â€“ğ±9 to be testing observations, ğ‘¦1â€“ ğ‘¦5 to be input variables and ğ‘¦6 to be the target variable. Hint: you can use scipy.stats.multivariate_normal for multivariate distribution calculus\n",
    "\n",
    "**a.) [3.5v] Learn a Bayesian classifier assuming: i) {ğ‘¦1, ğ‘¦2}, {ğ‘¦3, ğ‘¦4} and {ğ‘¦5} sets of independent variables (e.g., ğ‘¦1 â«« ğ‘¦3 yet ğ‘¦1 â«« ğ‘¦2), and  ii) ğ‘¦1 Ã— ğ‘¦2 âˆˆ â„2 is normally distributed.  Show all parameters (distributions and priors for subsequent testing).**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Training data for y1, y2, y3, y4, y5, y6\n",
    "train_data = np.array([\n",
    "    [0.24, 0.36, 1, 1, 0, 'A'],\n",
    "    [0.16, 0.48, 1, 0, 1, 'A'],\n",
    "    [0.32, 0.72, 0, 1, 2, 'A'],\n",
    "    [0.54, 0.11, 0, 0, 1, 'B'],\n",
    "    [0.66, 0.39, 0, 0, 0, 'B'],\n",
    "    [0.76, 0.28, 1, 0, 2, 'B'],\n",
    "    [0.41, 0.53, 0, 1, 1, 'B'],\n",
    "    \n",
    "])\n",
    "\n",
    "# Separate data by class A and B\n",
    "class_A_data = train_data[train_data[:, -1] == 'A'][:, :-1].astype(float)\n",
    "class_B_data = train_data[train_data[:, -1] == 'B'][:, :-1].astype(float)\n",
    "\n",
    "# Calculate means and covariance matrices for y1, y2, y3, y4, y5 in class A and B\n",
    "mean_A = np.mean(class_A_data, axis=0)\n",
    "mean_B = np.mean(class_B_data, axis=0)\n",
    "cov_A = np.cov(class_A_data, rowvar=False)\n",
    "cov_B = np.cov(class_B_data, rowvar=False)\n",
    "\n",
    "# Assuming ğ‘¦1 Ã— ğ‘¦2 is normally distributed, estimate the parameters for this bivariate normal distribution\n",
    "mean_y1_y2_A = mean_A[:2]  # Mean of {ğ‘¦1, ğ‘¦2} in class A\n",
    "mean_y1_y2_B = mean_B[:2]  # Mean of {ğ‘¦1, ğ‘¦2} in class B\n",
    "\n",
    "cov_y1_y2_A = cov_A[:2, :2]  # Covariance matrix of {ğ‘¦1, ğ‘¦2} in class A\n",
    "cov_y1_y2_B = cov_B[:2, :2]  # Covariance matrix of {ğ‘¦1, ğ‘¦2} in class B\n",
    "\n",
    "# Calculate prior probabilities\n",
    "prior_A = len(class_A_data) / len(train_data)\n",
    "prior_B = len(class_B_data) / len(train_data)\n",
    "\n",
    "# Define multivariate normal distributions for y1, y2, y3, y4, y5, and y6\n",
    "mvn_y1_A = multivariate_normal(mean=mean_A[0], cov=cov_A[0, 0])\n",
    "mvn_y2_A = multivariate_normal(mean=mean_A[1], cov=cov_A[1, 1])\n",
    "mvn_y3_A = multivariate_normal(mean=mean_A[2], cov=cov_A[2, 2])\n",
    "mvn_y4_A = multivariate_normal(mean=mean_A[3], cov=cov_A[3, 3])\n",
    "mvn_y5_A = multivariate_normal(mean=mean_A[4], cov=cov_A[4, 4])\n",
    "\n",
    "mvn_y1_B = multivariate_normal(mean=mean_B[0], cov=cov_B[0, 0])\n",
    "mvn_y2_B = multivariate_normal(mean=mean_B[1], cov=cov_B[1, 1])\n",
    "mvn_y3_B = multivariate_normal(mean=mean_B[2], cov=cov_B[2, 2])\n",
    "mvn_y4_B = multivariate_normal(mean=mean_B[3], cov=cov_B[3, 3])\n",
    "mvn_y5_B = multivariate_normal(mean=mean_B[4], cov=cov_B[4, 4])\n",
    "\n",
    "# Print the calculated parameters and prior probabilities\n",
    "print(\"Means and Covariance for y1, y2 in Class A:\")\n",
    "print(mean_A[:2])\n",
    "print(cov_A[:2, :2])\n",
    "print(\"\\nMeans and Covariance for y1, y2 in Class B:\")\n",
    "print(mean_B[:2])\n",
    "print(cov_B[:2, :2])\n",
    "print(\"\\nParameters for Bivariate Normal Distribution ğ‘¦1 Ã— ğ‘¦2 in Class A:\")\n",
    "print(\"Mean:\", mean_y1_y2_A)\n",
    "print(\"Covariance Matrix:\")\n",
    "print(cov_y1_y2_A)\n",
    "print(\"\\nParameters for Bivariate Normal Distribution ğ‘¦1 Ã— ğ‘¦2 in Class B:\")\n",
    "print(\"Mean:\", mean_y1_y2_B)\n",
    "print(\"Covariance Matrix:\")\n",
    "print(cov_y1_y2_B)\n",
    "print(\"\\nPrior Probabilities:\")\n",
    "print(\"P(y1):\", prior_A)\n",
    "print(\"P(y2):\", prior_B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.) [2.5v] Under a MAP assumption, classify each testing observation showing all your calculus.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.) [2v] Consider that the default decision threshold of ğœƒ = 0.5 can be adjusted according to ğ‘“(ğ±|ğœƒ) = { A ğ‘ƒ(A|ğ±) > ğœƒ B otherwise . Under a maximum likelihood assumption, what thresholds optimize testing accuracy?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Let ğ‘¦1 be the target numeric variable, ğ‘¦2-ğ‘¦6 be the input variables where ğ‘¦2 is binarized under an equal-width (equal-range) discretization. For the evaluation of regressors, consider a 3-fold cross-validation over the full dataset (ğ±1- ğ±9) without shuffling the observations.\n",
    "\n",
    "**a. [1v] Identify the observations and features per data fold after the binarization procedure.**\n",
    "\n",
    "**b. [4v] Consider a distance-weighted ğ‘˜NN with ğ‘˜ = 3, Hamming distance (ğ‘‘), and 1/ğ‘‘ weighting. Compute the MAE of this ğ‘˜NN regressor for the 1st iteration of the cross-validation (i.e. train observations have the lower indices).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming [7v]\n",
    "\n",
    "**Considering the column_diagnosis.arff dataset available at the course webpageâ€™s homework tab.\n",
    "Using sklearn, apply a 10-fold stratified cross-validation with shuffling (random_state=0) for the\n",
    "assessment of predictive models along this section.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "data = loadarff('column_diagnosis.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "df['class'] = df['class'].str.decode('utf-8')\n",
    "\n",
    "X = df.drop(columns=['class'])  \n",
    "y = df['class']   \n",
    "\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [3v]\n",
    "\n",
    "**Compare the performance of ğ‘˜NN with ğ‘˜ = 5 and naÃ¯ve Bayes with Gaussian assumption\n",
    "(consider all remaining parameters for each classifier as sklearnâ€™s default):\n",
    "a. Plot two boxplots with the fold accuracies for each classifier.\n",
    "b. Using scipy, test the hypothesis â€œğ‘˜NN is statistically superior to naÃ¯ve Bayes regarding\n",
    "accuracyâ€, asserting whether is true**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a) plot two boxplost with the fold accuracies for kNN with k=5 and naive Bayes with Gaussian assumptions\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(n_neighbors=5),\n",
    "    GaussianNB()\n",
    "]\n",
    "\n",
    "classifier_names = [\n",
    "    'kNN (k=5)',\n",
    "    'Naive Bayes (Gaussian)'\n",
    "]\n",
    "\n",
    "classifier_accuracies = []\n",
    "\n",
    "for classifier in classifiers:\n",
    "    accuracies = cross_val_score(classifier, X, y, cv=stratified_kfold)\n",
    "    classifier_accuracies.append(accuracies)\n",
    "\n",
    "plt.boxplot(classifier_accuracies)\n",
    "plt.xticks(np.arange(1, len(classifier_names) + 1), classifier_names)\n",
    "plt.xlabel('Classifier')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('10-fold Stratified Cross-Validation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b) Using scipy, test the hypothesis â€œğ‘˜NN is statistically superior to naÃ¯ve Bayes regarding accuracyâ€, asserting whether is true\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Perform a paired t-test\n",
    "p_value = stats.ttest_rel(classifier_accuracies[0], classifier_accuracies[1], alternative='greater').pvalue\n",
    "\n",
    "# Define the significance level\n",
    "alpha = 0.1\n",
    "\n",
    "# Determine if k-NN is statistically superior to Naive Bayes\n",
    "if p_value < alpha:\n",
    "    result = \"k-NN is statistically superior to Naive Bayes\"\n",
    "else:\n",
    "    result = \"There is no significant difference between k-NN and Naive Bayes\"\n",
    "\n",
    "# Print the results\n",
    "print(\"P-value:\", p_value)\n",
    "print(\"Hypothesis Test Result:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In accordance with the provided specifications, the p-value is 0.38. At commonly used significance levels (e.g., Î± = 0.1), it is not possible to reject the null hypothesis. Consequently, we cannot claim that the proposed hypothesis is valid. It's important to emphasize that we should avoid concluding that the given hypothesis is false or rejected without conducting further statistical tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) [2.5v]\n",
    "\n",
    "**Consider two ğ‘˜NN predictors with ğ‘˜ = 1 and ğ‘˜ = 5 (uniform weights, Euclidean distance,\n",
    "all remaining parameters as default). Plot the differences between the two cumulative confusion\n",
    "matrices of the predictors. Comment.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create two k-NN classifiers with k=1 and k=5\n",
    "knn_k1 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_k5 = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the classifiers to the data and make predictions\n",
    "knn_k1.fit(X, y)\n",
    "knn_k5.fit(X, y)\n",
    "y_pred_k1 = knn_k1.predict(X)\n",
    "y_pred_k5 = knn_k5.predict(X)\n",
    "\n",
    "# Calculate confusion matrices for both classifiers\n",
    "cm_k1 = confusion_matrix(y, y_pred_k1)\n",
    "cm_k5 = confusion_matrix(y, y_pred_k5)\n",
    "\n",
    "# Compute the difference between cumulative confusion matrices\n",
    "cm_diff = cm_k1 - cm_k5\n",
    "\n",
    "print (\"Confusion Matrix for k=1:\")\n",
    "print(cm_k1)\n",
    "print(\"\\nConfusion Matrix for k=5:\")\n",
    "print(cm_k5)\n",
    "print(\"\\nDifference in Confusion Matrices (k=1 - k=5):\")\n",
    "print(cm_diff)\n",
    "\n",
    "# Plot the differences\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm_diff, cmap='coolwarm', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title(\"Difference in Cumulative Confusion Matrices (k=1 - k=5)\")\n",
    "plt.xticks(np.arange(len(np.unique(y))), np.unique(y), rotation=45)\n",
    "plt.yticks(np.arange(len(np.unique(y))), np.unique(y))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [1.5v]\n",
    "\n",
    "**Considering the unique properties of column_diagnosis, identify three possible difficulties\n",
    "of naÃ¯ve Bayes when learning from the given dataset.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
