{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework I\n",
    "\n",
    "Joana Peixinho (ist1103335) & Miguel Fernandes (ist1103573)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Pen and Paper [11v]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [6v] Perform one epoch of the EM clustering algorithm and determine the new parameters. \n",
    "Hint: we suggest you to use numpy and scipy, however disclose the intermediary results step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "observations = np.array([\n",
    "    [1, 0.6, 0.1],\n",
    "    [0, -0.4, 0.8],\n",
    "    [0, 0.2, 0.5],\n",
    "    [1, 0.4, -0.1]\n",
    "])\n",
    "\n",
    "# Number of clusters\n",
    "K = 2\n",
    "\n",
    "\n",
    "pi = np.array([0.5, 0.5])\n",
    "p = np.array([0.3, 0.7])\n",
    "mu = np.array([[1, 1], [0, 0]])\n",
    "cov = np.array([[[2, 0.5], [0.5, 2]], [[1.5, 1], [1, 1.5]]])\n",
    "\n",
    "updated_pi = np.array([0.0, 0.0])\n",
    "updated_p = np.array([0.0, 0.0])\n",
    "updated_mu = np.array([[0.0, 0.0], [0.0, 0.0]])\n",
    "updated_cov = np.array([[[0.0, 0.0], [0.0, 0.0]], [[0.0, 0.0], [0.0, 0.0]]])\n",
    "# EM Clustering\n",
    "num_iterations = 1\n",
    "\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # Expectation step\n",
    "    cluster_probabilities = np.zeros((len(observations), K))\n",
    "    for k in range(K):\n",
    "        cluster_probabilities[:, k] = pi[k] * multivariate_normal.pdf(observations[:, 1:], mean=mu[k], cov=cov[k])\n",
    "        #To account for the Bernoulli distribution of y1\n",
    "        for i in range(len(observations)): \n",
    "            if observations[i, 0] == 0:\n",
    "                cluster_probabilities[i, k] *= (1 - p[k])\n",
    "            else:  \n",
    "                cluster_probabilities[i, k] *= p[k]\n",
    "    \n",
    "    print(\"Posteriori probabilities, before normalization:\")\n",
    "    print(cluster_probabilities)\n",
    "    \n",
    "    cluster_probabilities /= cluster_probabilities.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    print(\"Posteriori probabilities, after normalization:\")\n",
    "    print(cluster_probabilities)\n",
    "\n",
    "    # Maximization step\n",
    "    Nk = cluster_probabilities.sum(axis=0)\n",
    "    updated_pi = Nk / len(observations)\n",
    "    updated_p = cluster_probabilities.T.dot(observations[:, 0]) / Nk\n",
    " \n",
    " \n",
    "    for k in range(K):\n",
    "        updated_mu[k] = cluster_probabilities[:, k].dot(observations[:, 1:])\n",
    "    updated_mu = updated_mu / Nk[:, np.newaxis]\n",
    "    \n",
    "\n",
    "    for k in range(K):\n",
    "        x_minus_mu = observations[:, 1:] - updated_mu[k]\n",
    "        updated_cov[k] = np.dot(x_minus_mu.T, (x_minus_mu * cluster_probabilities[:, k][:, np.newaxis])) / Nk[k]\n",
    "\n",
    "\n",
    "print(\"Priors (Pi) atualizados:\")\n",
    "print(updated_pi)\n",
    "print(\"Probabilidades (p) atualizadas:\")\n",
    "print(updated_p)\n",
    "print(\"M√©dias mu_1 e mu_2 atualizadas:\")\n",
    "print(updated_mu)\n",
    "print(\"Matrizes de Covari√¢ncia atualizadas:\")\n",
    "print(updated_cov)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)  [2v] Given the new observation, ùê±ùëõùëíùë§ , determine the cluster memberships (posteriors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# New observation\n",
    "x_new = np.array([1, 0.3, 0.7])\n",
    "\n",
    "# Expectation step\n",
    "cluster_probabilities_x_new = np.zeros(K)\n",
    "\n",
    "for k in range(K):\n",
    "    cluster_probabilities_x_new[k] = updated_pi[k] * multivariate_normal.pdf(x_new[1:], mean=updated_mu[k], cov=updated_cov[k])\n",
    "    if x_new[0] == 0:\n",
    "        cluster_probabilities_x_new[k] *= (1 - updated_p[k])\n",
    "    else:  \n",
    "        cluster_probabilities_x_new[k] *= updated_p[k]\n",
    "\n",
    "cluster_probabilities_x_new /= cluster_probabilities_x_new.sum()\n",
    "\n",
    "print(\"Cluster Probabilities for the new observation:\")\n",
    "print(cluster_probabilities_x_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [2.5v] Performing a hard assignment of observations to clusters under a ML assumption, identify the silhouette of the larger cluster under a Manhattan distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0. , -0.4,  0.8]), array([0. , 0.2, 0.5])]\n",
      "[array([1. , 0.6, 0.1]), array([ 1. ,  0.4, -0.1])]\n",
      "Pairwise Manhattan Distances:\n",
      "[[0.  2.7 1.8 0.4]\n",
      " [2.7 0.  0.9 2.7]\n",
      " [1.8 0.9 0.  1.8]\n",
      " [0.4 2.7 1.8 0. ]]\n",
      "Pontua√ß√µes de Silhueta para Cluster 0:\n",
      "[0.66666667 0.5       ]\n",
      "Pontua√ß√µes de Silhueta para Cluster 1:\n",
      "[0.82222222 0.82222222]\n",
      "Average silhouette score for cluster 0:\n",
      "0.5833333333333333\n",
      "Average silhouette score for cluster 1:\n",
      "0.8222222222222223\n",
      "Likelihoods for each observation being in each cluster:\n",
      "[[0.29671191 0.99604626]\n",
      " [1.15728255 0.08001945]\n",
      " [1.31426785 0.40955753]\n",
      " [0.02661759 0.7587361 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import numpy as np\n",
    "\n",
    "# Calculate likelihoods\n",
    "updated_likelihoods = np.zeros((len(observations), K))\n",
    "updated_class_observations = observations\n",
    "\n",
    "for k in range(K):\n",
    "    updated_likelihoods[:, k] = multivariate_normal.pdf(observations[:, 1:], mean=updated_mu[k], cov=updated_cov[k])\n",
    "    \n",
    "    for i in range(len(observations)): \n",
    "        if observations[i, 0] == 0:\n",
    "            updated_likelihoods[i, k] *= (1 - p[k])\n",
    "        else:  \n",
    "            updated_likelihoods[i, k] *= p[k]\n",
    "\n",
    "# Update class of observations (the cluster they are in)\n",
    "for i in range(len(updated_class_observations)):\n",
    "    if updated_likelihoods[i][0] >= updated_likelihoods[i][1]:\n",
    "        updated_class_observations[i][0] = 0\n",
    "\n",
    "cluster_0 = [obs for obs in updated_class_observations if obs[0] == 0]\n",
    "cluster_1 = [obs for obs in updated_class_observations if obs[0] == 1]\n",
    "\n",
    "print(cluster_0)\n",
    "print(cluster_1)\n",
    "\n",
    "\n",
    "# Calculate pairwise distances using Manhattan distance\n",
    "pairwise_manhattan_distances = pairwise_distances(observations, metric=\"manhattan\")\n",
    "\n",
    "# Calculate silhouette scores\n",
    "silhouette_scores = silhouette_samples(pairwise_manhattan_distances, updated_class_observations[:, 0], metric=\"precomputed\")\n",
    "\n",
    "print (\"Pairwise Manhattan Distances:\")\n",
    "print(pairwise_manhattan_distances)\n",
    "\n",
    "# Separar as pontua√ß√µes de silhueta por cluster\n",
    "silhouette_scores_cluster_0 = silhouette_scores[updated_class_observations[:, 0] == 0]\n",
    "silhouette_scores_cluster_1 = silhouette_scores[updated_class_observations[:, 0] == 1]\n",
    "\n",
    "print(\"Pontua√ß√µes de Silhueta para Cluster 0:\")\n",
    "print(silhouette_scores_cluster_0)\n",
    "print(\"Pontua√ß√µes de Silhueta para Cluster 1:\")\n",
    "print(silhouette_scores_cluster_1)\n",
    "\n",
    "#average silhouette score for cluster 0\n",
    "print(\"Average silhouette score for cluster 0:\")\n",
    "print(silhouette_scores_cluster_0.mean())\n",
    "\n",
    "#average silhouette score for cluster 1\n",
    "print(\"Average silhouette score for cluster 1:\")\n",
    "print(silhouette_scores_cluster_1.mean())\n",
    "\n",
    "\n",
    "\n",
    "# Print likelihoods, observations in each cluster, and silhouette scores\n",
    "print(\"Likelihoods for each observation being in each cluster:\")\n",
    "print(updated_likelihoods)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) [0.5v] Knowing the purity of the clustering solution is 0.75, identify the number of possible classes (ground truth).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming [9v]\n",
    "\n",
    "Recall the `column_diagnosis.arff` dataset from previous homeworks. For the following exercises,\n",
    "normalize the data using sklearn‚Äôs MinMaxScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io.arff import loadarff\n",
    "\n",
    "# Reading the ARFF file\n",
    "data = loadarff('column_diagnosis.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "df['class'] = df['class'].str.decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) \n",
    "[4v] Using sklearn, apply k-means clustering fully unsupervisedly on the normalized data with\n",
    "ùëò ‚àà {2,3,4,5} (random=0 and remaining parameters as default). Assess the silhouette and purity of\n",
    "the produced solutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, metrics, cluster, mixture\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "k = [2, 3, 4, 5]\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "\n",
    "X, y = df.drop('class', axis=1), df['class']\n",
    "X_scaled = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "for i in k:\n",
    "    kmeans = cluster.KMeans(n_clusters=i, random_state=0).fit(X_scaled)\n",
    "    print(\"K-Means Clustering with k = \" + str(i))\n",
    "    print(\"Silhouette Score: \" + str(metrics.silhouette_score(X_scaled, kmeans.labels_)))\n",
    "    print(\"Purity Score: \" + str(purity_score(y, kmeans.labels_)))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)\n",
    "[2v] Consider the application of PCA after the data normalization:\n",
    "i. Identify the variability explained by the top two principal components.\n",
    "ii. For each one of these two components, sort the input variables by relevance by\n",
    "inspecting the absolute weights of the linear projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fit PCA to the normalized data\n",
    "pca = PCA(svd_solver='full')\n",
    "pca = pca.fit(X_scaled)\n",
    "\n",
    "# 2i. Variability explained by the top two principal components\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "print(\"Variability explained by the top two principal components: {:.2f}%\".format(sum(explained_variance_ratio[:2]) * 100))\n",
    "\n",
    "# 2ii. Sort input variables by relevance in the top two components\n",
    "sorted_variables_pc1 = np.argsort(np.abs(pca.components_[0]))[::-1]\n",
    "sorted_variables_pc2 = np.argsort(np.abs(pca.components_[1]))[::-1]\n",
    "\n",
    "\n",
    "# List the variables by relevance in the top two components\n",
    "print(\"Top variables for PC1:\")\n",
    "for i, var_index in enumerate(sorted_variables_pc1):\n",
    "    print(f\"{i+1}. {df.columns[var_index]}\")\n",
    "\n",
    "print(\"\\nTop variables for PC2:\")\n",
    "for i, var_index in enumerate(sorted_variables_pc2):\n",
    "    print(f\"{i+1}. {df.columns[var_index]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) \n",
    "[2v] Visualize side-by-side the data using: i) the ground diagnoses, and ii) the previously learned\n",
    "ùëò = 3 clustering solution. To this end, projected the normalized data onto a 2-dimensional data\n",
    "space using PCA and then color observations using the reference and cluster annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "kmeans = cluster.KMeans(n_clusters=3, random_state=0).fit(X_scaled)\n",
    "#PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "num = [0 if x == 'Hernia' else 1 if x == 'Spondylolisthesis' else 2 for x in y]\n",
    "plt.scatter(X_2d[:, 0], X_2d[:, 1], c=num, cmap='viridis')\n",
    "plt.title(\"Original Data (Diagnoses)\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_2d[:, 0], X_2d[:, 1], c=kmeans.labels_, cmap='viridis')\n",
    "plt.title(\"K-Means Clustering\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4)\n",
    "[1v] Considering the results from questions (1) and (3), identify two ways on how clustering can\n",
    "be used to characterize the population of ill and healthy individuals\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering can be used to characterize the population of ill and healthy individuals in two ways:\n",
    "\n",
    "Identifying Subgroups: Clustering helps identify different subgroups within the ill or healthy populations based on common characteristics. This can help identify the characteristics that are most closely associated with the illness.\n",
    "\n",
    "Risk Assessment: Clustering can also be used to assess the risk of developing an illness. For example, if a person is in a cluster with many ill people, they may be at higher risk of developing the illness than someone in a cluster with few ill people."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
