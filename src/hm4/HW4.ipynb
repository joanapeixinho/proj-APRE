{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework I\n",
    "\n",
    "Joana Peixinho (ist1103335) & Miguel Fernandes (ist1103573)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Pen and Paper [11v]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [6v] Perform one epoch of the EM clustering algorithm and determine the new parameters. \n",
    "Hint: we suggest you to use numpy and scipy, however disclose the intermediary results step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "observations = np.array([\n",
    "    [1, 0.6, 0.1],\n",
    "    [0, -0.4, 0.8],\n",
    "    [0, 0.2, 0.5],\n",
    "    [1, 0.4, -0.1]\n",
    "])\n",
    "\n",
    "# Number of clusters\n",
    "K = 2\n",
    "\n",
    "\n",
    "pi = np.array([0.5, 0.5])\n",
    "p = np.array([0.3, 0.7])\n",
    "mu = np.array([[1, 1], [0, 0]])\n",
    "cov = np.array([[[2, 0.5], [0.5, 2]], [[1.5, 1], [1, 1.5]]])\n",
    "\n",
    "updated_mu = np.array([[0.0, 0.0], [0.0, 0.0]])\n",
    "updated_pi = np.array([0.0, 0.0])\n",
    "updated_cov = np.array([[[0.0, 0.0], [0.0, 0.0]], [[0.0, 0.0], [0.0, 0.0]]])\n",
    "# EM Clustering\n",
    "num_iterations = 1\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # Expectation step\n",
    "    cluster_probabilities = np.zeros((len(observations), K))\n",
    "    for k in range(K):\n",
    "        cluster_probabilities[:, k] = pi[k] * multivariate_normal.pdf(observations[:, 1:], mean=mu[k], cov=cov[k])\n",
    "    \n",
    "    cluster_probabilities /= cluster_probabilities.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Maximization step\n",
    "    Nk = cluster_probabilities.sum(axis=0)\n",
    "    updated_pi = Nk / len(observations)\n",
    "    p = cluster_probabilities.T.dot(observations[:, 0]) / Nk\n",
    " \n",
    " \n",
    "    for k in range(K):\n",
    "        updated_mu[k] = cluster_probabilities[:, k].dot(observations[:, 1:])\n",
    "    updated_mu = updated_mu / Nk[:, np.newaxis]\n",
    "    \n",
    "\n",
    "    for k in range(K):\n",
    "        x_minus_mu = observations[:, 1:] - updated_mu[k]\n",
    "        updated_cov[k] = np.dot(x_minus_mu.T, (x_minus_mu * cluster_probabilities[:, k][:, np.newaxis])) / Nk[k]\n",
    "\n",
    "\n",
    "print(\"Priors atualizados:\")\n",
    "print(updated_pi)\n",
    "print(\"Médias mu_1 e mu_2 atualizadas:\")\n",
    "print(updated_mu)\n",
    "print(\"Matrizes de Covariância atualizadas:\")\n",
    "print(updated_cov)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)  [2v] Given the new observation, 𝐱𝑛𝑒𝑤 , determine the cluster memberships (posteriors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Given code (with updated variables)\n",
    "# ...\n",
    "\n",
    "# New observation\n",
    "x_new = np.array([1, 0.3, 0.7])\n",
    "\n",
    "# Expectation step\n",
    "cluster_probabilities_x_new = np.zeros(K)\n",
    "\n",
    "for k in range(K):\n",
    "    cluster_probabilities_x_new[k] = pi[k] * multivariate_normal.pdf(x_new[1:], mean=updated_mu[k], cov=updated_cov[k])\n",
    "\n",
    "cluster_probabilities_x_new /= cluster_probabilities_x_new.sum()\n",
    "\n",
    "print(\"Cluster Probabilities for the new observation:\")\n",
    "print(cluster_probabilities_x_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [2.5v] Performing a hard assignment of observations to clusters under a ML assumption, identify the silhouette of the larger cluster under a Manhattan distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Calculate likelihoods\n",
    "updated_likelihoods = np.zeros((len(observations), K))\n",
    "updated_class_observations = observations\n",
    "number_of_observations_in_cluster0 = 0\n",
    "biggest_cluster_index = 0\n",
    "\n",
    "for k in range(K):\n",
    "    updated_likelihoods[:, k] = multivariate_normal.pdf(observations[:, 1:], mean=updated_mu[k], cov=updated_cov[k])\n",
    "\n",
    "# Update class of observations (the cluster they are in)\n",
    "for i in range(len(updated_class_observations)):\n",
    "    if updated_likelihoods[i][0] >= updated_likelihoods[i][1]:\n",
    "        updated_class_observations[i][0] = 0\n",
    "\n",
    "\n",
    "\n",
    "# calculate silhouette for each observation\n",
    "silhouetts = [[], []]\n",
    "for obs_index in range(len(updated_class_observations)):\n",
    "    a_i=0\n",
    "    b_i=0\n",
    "    obs_count_a=0\n",
    "    obs_count_b=0\n",
    "    for obs in updated_class_observations:\n",
    "        if any(obs[1:] != updated_class_observations[obs_index][1:]):\n",
    "            if obs[0] == updated_class_observations[obs_index][0]:\n",
    "                a_i += np.linalg.norm(obs[1:]- updated_class_observations[obs_index][1:])\n",
    "                obs_count_a += 1\n",
    "            else:\n",
    "                b_i += np.linalg.norm(obs[1:]- updated_class_observations[obs_index][1:])\n",
    "                obs_count_b += 1\n",
    "    a_i /= obs_count_a\n",
    "    b_i /= obs_count_b\n",
    "    \n",
    "    if a_i < b_i:\n",
    "        silhouetts[int(updated_class_observations[obs_index][0])].append(1 - a_i/b_i)\n",
    "    else:\n",
    "        silhouetts[int(updated_class_observations[obs_index][0])].append(b_i/a_i - 1)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"Likelihoods for each observation being in each cluster:\")\n",
    "print(updated_likelihoods)\n",
    "print(f\"Observations in cluster 0: \\n {[obs for obs in updated_class_observations if obs[0] == 0]}\")\n",
    "print(f\"Observations in cluster 1: \\n {[obs for obs in updated_class_observations if obs[0] == 1]}\")\n",
    "print(f\"Silhouette for cluster 0: {np.mean(silhouetts[0])}\")\n",
    "print(f\"Silhouette for cluster 1: {np.mean(silhouetts[1])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) [0.5v] Knowing the purity of the clustering solution is 0.75, identify the number of possible classes (ground truth).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming [9v]\n",
    "\n",
    "Recall the `column_diagnosis.arff` dataset from previous homeworks. For the following exercises,\n",
    "normalize the data using sklearn’s MinMaxScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io.arff import loadarff\n",
    "\n",
    "# Reading the ARFF file\n",
    "data = loadarff('column_diagnosis.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "df['class'] = df['class'].str.decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) \n",
    "[4v] Using sklearn, apply k-means clustering fully unsupervisedly on the normalized data with\n",
    "𝑘 ∈ {2,3,4,5} (random=0 and remaining parameters as default). Assess the silhouette and purity of\n",
    "the produced solutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, metrics, cluster, mixture\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "k = [2, 3, 4, 5]\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    # return purity\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "\n",
    "#normalize the data\n",
    "X, y = df.drop('class', axis=1), df['class']\n",
    "X_scaled = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# K-Means for different values of K and print the results\n",
    "\n",
    "for i in k:\n",
    "    kmeans = cluster.KMeans(n_clusters=i, random_state=0).fit(X_scaled)\n",
    "    print(\"K-Means with k = \" + str(i))\n",
    "    print(\"Silhouette Score: \" + str(metrics.silhouette_score(X_scaled, kmeans.labels_)))\n",
    "    print(\"Purity Score: \" + str(purity_score(y, kmeans.labels_)))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)\n",
    "[2v] Consider the application of PCA after the data normalization:\n",
    "i. Identify the variability explained by the top two principal components.\n",
    "ii. For each one of these two components, sort the input variables by relevance by\n",
    "inspecting the absolute weights of the linear projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fit PCA to the normalized data\n",
    "pca = PCA(svd_solver='full')\n",
    "pca = pca.fit(X_scaled)\n",
    "\n",
    "# 2i. Variability explained by the top two principal components\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "print(\"Variability explained by the top two principal components: {:.2f}%\".format(sum(explained_variance_ratio[:2]) * 100))\n",
    "\n",
    "# 2ii. Sort input variables by relevance in the top two components\n",
    "sorted_variables_pc1 = np.argsort(np.abs(pca.components_[0]))[::-1]\n",
    "sorted_variables_pc2 = np.argsort(np.abs(pca.components_[1]))[::-1]\n",
    "\n",
    "\n",
    "# List the variables by relevance in the top two components\n",
    "print(\"Top variables for PC1:\")\n",
    "for i, var_index in enumerate(sorted_variables_pc1):\n",
    "    print(f\"{i+1}. {df.columns[var_index]}\")\n",
    "\n",
    "print(\"\\nTop variables for PC2:\")\n",
    "for i, var_index in enumerate(sorted_variables_pc2):\n",
    "    print(f\"{i+1}. {df.columns[var_index]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) \n",
    "[2v] Visualize side-by-side the data using: i) the ground diagnoses, and ii) the previously learned\n",
    "𝑘 = 3 clustering solution. To this end, projected the normalized data onto a 2-dimensional data\n",
    "space using PCA and then color observations using the reference and cluster annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "kmeans = cluster.KMeans(n_clusters=3, random_state=0).fit(X_scaled)\n",
    "#PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "num = [0 if x == 'Hernia' else 1 if x == 'Spondylolisthesis' else 2 for x in y]\n",
    "plt.scatter(X_2d[:, 0], X_2d[:, 1], c=num, cmap='viridis')\n",
    "plt.title(\"Original Data (Diagnoses)\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_2d[:, 0], X_2d[:, 1], c=kmeans.labels_, cmap='viridis')\n",
    "plt.title(\"K-Means Clustering\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4)\n",
    "[1v] Considering the results from questions (1) and (3), identify two ways on how clustering can\n",
    "be used to characterize the population of ill and healthy individuals\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering can be used to characterize the population of ill and healthy individuals in two ways:\n",
    "\n",
    "Identifying Subgroups: Clustering helps identify different subgroups within the ill or healthy populations based on common characteristics. This can help identify the characteristics that are most closely associated with the illness.\n",
    "\n",
    "Risk Assessment: Clustering can also be used to assess the risk of developing an illness. For example, if a person is in a cluster with many ill people, they may be at higher risk of developing the illness than someone in a cluster with few ill people."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
