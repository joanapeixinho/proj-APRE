{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework I\n",
    "\n",
    "Joana Peixinho (ist1103335) & Miguel Fernandes (ist1103573)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Pen and Paper [11v]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [6v] Perform one epoch of the EM clustering algorithm and determine the new parameters. \n",
    "Hint: we suggest you to use numpy and scipy, however disclose the intermediary results step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "observations = np.array([\n",
    "    [1, 0.6, 0.1],\n",
    "    [0, -0.4, 0.8],\n",
    "    [0, 0.2, 0.5],\n",
    "    [1, 0.4, -0.1]\n",
    "])\n",
    "\n",
    "\n",
    "K = 2\n",
    "\n",
    "\n",
    "pi = np.array([0.5, 0.5])\n",
    "p = np.array([0.3, 0.7])\n",
    "mu = np.array([[1, 1], [0, 0]])\n",
    "cov = np.array([[[2, 0.5], [0.5, 2]], [[1.5, 1], [1, 1.5]]])\n",
    "\n",
    "# EM Clustering\n",
    "num_iterations = 1\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # Expectation step\n",
    "    cluster_probabilities = np.zeros((len(observations), K))\n",
    "    for k in range(K):\n",
    "        cluster_probabilities[:, k] = pi[k] * multivariate_normal.pdf(observations[:, 1:], mean=mu[k], cov=cov[k])\n",
    "    \n",
    "    cluster_probabilities /= cluster_probabilities.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Maximization step\n",
    "    Nk = cluster_probabilities.sum(axis=0)\n",
    "    pi = Nk / len(observations)\n",
    "    p = cluster_probabilities.T.dot(observations[:, 0]) / Nk\n",
    "    for k in range(K):\n",
    "        x_minus_mu = observations[:, 1:] - p[k]\n",
    "        cov[k] = np.dot(x_minus_mu.T, (x_minus_mu * cluster_probabilities[:, k][:, np.newaxis])) / Nk[k]\n",
    "\n",
    "\n",
    "print(\"Probabilidades dos Clusters:\")\n",
    "print(pi)\n",
    "print(\"Probabilidade p1 e p2:\")\n",
    "print(p)\n",
    "print(\"Matrizes de Covari√¢ncia:\")\n",
    "print(cov)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)  [2v] Given the new observation, ùê±ùëõùëíùë§ , determine the cluster memberships (posteriors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [2.5v] Performing a hard assignment of observations to clusters under a ML assumption, identify the silhouette of the larger cluster under a Manhattan distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) [0.5v] Knowing the purity of the clustering solution is 0.75, identify the number of possible classes (ground truth).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming [9v]\n",
    "\n",
    "Recall the `column_diagnosis.arff` dataset from previous homeworks. For the following exercises,\n",
    "normalize the data using sklearn‚Äôs MinMaxScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io.arff import loadarff\n",
    "\n",
    "# Reading the ARFF file\n",
    "data = loadarff('column_diagnosis.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "df['class'] = df['class'].str.decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) \n",
    "[4v] Using sklearn, apply k-means clustering fully unsupervisedly on the normalized data with\n",
    "ùëò ‚àà {2,3,4,5} (random=0 and remaining parameters as default). Assess the silhouette and purity of\n",
    "the produced solutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, metrics, cluster, mixture\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "k = [2, 3, 4, 5]\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    # return purity\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "\n",
    "#normalize the data\n",
    "X, y = df.drop('class', axis=1), df['class']\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# K-Means for different values of K and print the results\n",
    "\n",
    "for i in k:\n",
    "    kmeans = cluster.KMeans(n_clusters=i, random_state=0).fit(X)\n",
    "    print(\"K-Means with k = \" + str(i))\n",
    "    print(\"Silhouette Score: \" + str(metrics.silhouette_score(X, kmeans.labels_)))\n",
    "    print(\"Purity Score: \" + str(purity_score(y, kmeans.labels_)))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)\n",
    "[2v] Consider the application of PCA after the data normalization:\n",
    "i. Identify the variability explained by the top two principal components.\n",
    "ii. For each one of these two components, sort the input variables by relevance by\n",
    "inspecting the absolute weights of the linear projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) \n",
    "[2v] Visualize side-by-side the data using: i) the ground diagnoses, and ii) the previously learned\n",
    "ùëò = 3 clustering solution. To this end, projected the normalized data onto a 2-dimensional data\n",
    "space using PCA and then color observations using the reference and cluster annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4)\n",
    "[1v] Considering the results from questions (1) and (3), identify two ways on how clustering can\n",
    "be used to characterize the population of ill and healthy individuals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
