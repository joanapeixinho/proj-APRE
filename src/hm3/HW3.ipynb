{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework III\n",
    "\n",
    "Joana Peixinho (ist1103335) & Miguel Fernandes (ist1103573)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Pen and Paper [12v]\n",
    "\n",
    "For questions in this group, show your numerical results with 5 decimals or scientific notation.\n",
    "Hint: we highly recommend the use of numpy (e.g., linalg.pinv for inverse) or other programmatic\n",
    "facilities to support the calculus involved in both questions (1) and (2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Consider the problem of learning a regression model from 4 bivariate observations{(0.7‚àí0.3) , (0.40.5) , (‚àí0.20.8) , (‚àí0.40.3)} with targets (0.8, 0.6, 0.3, 0.3).\n",
    "\n",
    "**a.) . [4v] Given the radial basis function, ùúôùëó(ùë•) = ùëíùë•ùëù (‚àí(‚Äñùê±‚àíùíÑùëó‚Äñ^2)/2), that transforms the original space onto a new space characterized by the similarity of the original observations to the following data points, {ùíÑ1 = (0,0) , ùíÑ2 = (1,‚àí1) , ùíÑ3 = (‚àí1,1)}. Learn the Ridge regression (ùëô2 regularization) using the closed solution with ùúÜ = 0.1.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix X:\n",
      "[[1.      0.74826 0.74826 0.10127]\n",
      " [1.      0.81465 0.27117 0.33121]\n",
      " [1.      0.71177 0.09633 0.71177]\n",
      " [1.      0.8825  0.16122 0.65377]]\n",
      "Vector y:\n",
      "[[0.8]\n",
      " [0.6]\n",
      " [0.3]\n",
      " [0.3]]\n",
      "Weights:\n",
      "[[ 0.33914]\n",
      " [ 0.19945]\n",
      " [ 0.40096]\n",
      " [-0.296  ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given data\n",
    "observations = np.array([[0.7, -0.3], [0.4, 0.5], [-0.2, 0.8], [-0.4, 0.3]])\n",
    "targets = np.array([0.8, 0.6, 0.3, 0.3])\n",
    "c_points = np.array([[0, 0], [1, -1], [-1, 1]])\n",
    "Œª = 0.1\n",
    "\n",
    "# Step 1: Compute radial basis function features\n",
    "def radial_basis_function(x, c):\n",
    "    return np.exp(-np.linalg.norm(x - c)**2 / 2)\n",
    "\n",
    "phi = np.array([[radial_basis_function(x, c) for c in c_points] for x in observations])\n",
    "\n",
    "# Include a column of ones for the bias term in the feature matrix\n",
    "X = np.hstack((np.ones((observations.shape[0], 1)), phi))\n",
    "\n",
    "# Step 2: Set up Ridge regression problem\n",
    "y = targets.reshape(-1, 1)\n",
    "\n",
    "# Step 3: Solve for weights\n",
    "I = np.identity(X.shape[1])\n",
    "w = np.linalg.inv(X.T @ X + Œª*I) @ X.T @ y\n",
    "\n",
    "# Extract the bias term w0\n",
    "w0 = w[0, 0]\n",
    "\n",
    "# Extract the other weights (w1, w2, ..., wn)\n",
    "weights = w[1:]\n",
    "\n",
    "# Print the weights\n",
    "print(\"Matrix X:\")\n",
    "print(np.around(X, decimals=5))\n",
    "print(\"Vector y:\")\n",
    "print(y)\n",
    "print(\"Weights:\")\n",
    "print(np.around(w, decimals=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.) [2v] Compute the training RMSE for the learnt regression.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted targets:\n",
      "[[0.75844]\n",
      " [0.51232]\n",
      " [0.30905]\n",
      " [0.38629]]\n",
      "RMSE: 0.06508\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Root Mean Square Error\n",
    "y_pred = X @ w\n",
    "RMSE = np.sqrt(np.mean((y - y_pred)**2))\n",
    "print(\"Predicted targets:\")\n",
    "print(np.around(y_pred, decimals=5))\n",
    "print(\"RMSE:\", np.around(RMSE, decimals=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) [6v] Consider a MLP classifier of three outcomes ‚Äì ùê¥, ùêµ and ùê∂ ‚Äì characterized by the weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Rest of the Exercise](./I_2_exercise.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/miguel/Univ/third-year/Apre/proj-APRE/src/hm3/HW3.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/miguel/Univ/third-year/Apre/proj-APRE/src/hm3/HW3.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m         target_value \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/miguel/Univ/third-year/Apre/proj-APRE/src/hm3/HW3.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(z \u001b[39m-\u001b[39m target_value)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/miguel/Univ/third-year/Apre/proj-APRE/src/hm3/HW3.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m loss1 \u001b[39m=\u001b[39m loss(z3, target1)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/miguel/Univ/third-year/Apre/proj-APRE/src/hm3/HW3.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m loss2 \u001b[39m=\u001b[39m loss(z3, target2)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/miguel/Univ/third-year/Apre/proj-APRE/src/hm3/HW3.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# Backward Pass (Gradient Calculation)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'z3' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the parameters\n",
    "W1 = np.array([[1, 1, 1, 1], [1, 1, 2, 1], [1, 1, 1, 1]])\n",
    "b1 = np.array([1, 1, 1])\n",
    "W2 = np.array([[1, 4, 1], [1, 1, 1]])\n",
    "b2 = np.array([1, 1])\n",
    "W3 = np.array([[1, 1], [3, 1], [1, 1]])\n",
    "b3 = np.array([1, 1, 1])\n",
    "\n",
    "x1 = np.array([1, 1, 1, 1])\n",
    "x2 = np.array([1, 0, 0, -1])\n",
    "\n",
    "target1 = 'B'\n",
    "target2 = 'A'\n",
    "\n",
    "# Activation function\n",
    "def activation(x):\n",
    "    return np.tanh(0.5*x - 2)\n",
    "\n",
    "\n",
    "Z1, X1, Z2, X2, Z3 = [], [], [], [], []\n",
    "\n",
    "### Forward Pass\n",
    "# for x1\n",
    "Z1.append(W1 @ x1 + b1)\n",
    "X1.append(activation(Z1[-1]))\n",
    "# for x2\n",
    "Z1.append(W1 @ x2 + b1)\n",
    "X1.append(activation(Z1[-1]))\n",
    "\n",
    "# for x1\n",
    "Z2.append(W2 @ X1[0] + b2)\n",
    "X2.append(activation(Z2[-1]))\n",
    "# for x2\n",
    "Z2.append(W2 @ X1[1] + b2)\n",
    "X2.append(activation(Z2[-1]))\n",
    "\n",
    "# for x1\n",
    "Z3.append(W3 @ X2[0] + b3)\n",
    "# for x2\n",
    "Z3.append(W3 @ X2[1] + b3)\n",
    "\n",
    "# Calculate Loss\n",
    "def loss(z, target):\n",
    "    if target == 'A':\n",
    "        target_value = np.array([1, 0, 0])\n",
    "    elif target == 'B':\n",
    "        target_value = np.array([0, 1, 0])\n",
    "    elif target == 'C':\n",
    "        target_value = np.array([0, 0, 1])\n",
    "    return 0.5 * np.linalg.norm(z - target_value)**2\n",
    "\n",
    "loss1 = loss(Z3, target1)\n",
    "loss2 = loss(Z3, target2)\n",
    "\n",
    "# Backward Pass (Gradient Calculation)\n",
    "dLoss_dz3 = Z3 - (np.array([1, 0, 0]) if target2 == 'A' else np.array([0, 1, 0]))\n",
    "\n",
    "dLoss_dW3 = np.outer(dLoss_dz3, a2)\n",
    "dLoss_db3 = dLoss_dz3\n",
    "\n",
    "dLoss_da2 = np.dot(dLoss_dz3, W3)\n",
    "dLoss_dz2 = dLoss_da2 * 0.5 * (1 - np.tanh(0.5*z2 - 2)**2)\n",
    "\n",
    "dLoss_dW2 = np.outer(dLoss_dz2, a1)\n",
    "dLoss_db2 = dLoss_dz2\n",
    "\n",
    "dLoss_da1 = np.dot(dLoss_dz2, W2)\n",
    "dLoss_dz1 = dLoss_da1 * 0.5 * (1 - np.tanh(0.5*z1 - 2)**2)\n",
    "\n",
    "dLoss_dW1 = np.outer(dLoss_dz1, x1)\n",
    "dLoss_db1 = dLoss_dz1\n",
    "\n",
    "# Update Weights and Biases\n",
    "learning_rate = 0.1\n",
    "\n",
    "W3 -= learning_rate * dLoss_dW3\n",
    "b3 -= learning_rate * dLoss_db3\n",
    "\n",
    "W2 -= learning_rate * dLoss_dW2\n",
    "b2 -= learning_rate * dLoss_db2\n",
    "\n",
    "W1 -= learning_rate * dLoss_dW1\n",
    "b1 -= learning_rate * dLoss_db1\n",
    "\n",
    "# Print the updated weights and biases\n",
    "print(\"Updated W1:\")\n",
    "print(W1)\n",
    "print(\"Updated b1:\")\n",
    "print(b1)\n",
    "print(\"Updated W2:\")\n",
    "print(W2)\n",
    "print(\"Updated b2:\")\n",
    "print(b2)\n",
    "print(\"Updated W3:\")\n",
    "print(W3)\n",
    "print(\"Updated b3:\")\n",
    "print(b3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming [8v]\n",
    "\n",
    "**Consider the winequality-red.csv dataset (available at the webpage) where the goal is to estimate\n",
    "the quality (sensory appreciation) of a wine based on physicochemical inputs.\n",
    "Using a 80-20 training-test split with a fixed seed (random_state=0), you are asked to learn MLP\n",
    "regressors to answer the following questions.\n",
    "Given their stochastic behavior, average the performance of each MLP from 10 runs\n",
    "(for reproducibility consider seeding the MLPs with random_state ‚àà {1. .10}).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (1023, 11) (1023,)\n",
      "Validation set: (256, 11) (256,)\n",
      "Testing set: (320, 11) (320,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"winequality-red.csv\", sep=\";\")\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=0)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=0)  # Split further for validation\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y) for all sets\n",
    "X_train = train.drop(columns=['quality'])\n",
    "y_train = train['quality']\n",
    "X_val = val.drop(columns=['quality'])\n",
    "y_val = val['quality']\n",
    "X_test = test.drop(columns=['quality'])\n",
    "y_test = test['quality']\n",
    "\n",
    "print(\"Training set:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set:\", X_val.shape, y_val.shape)\n",
    "print(\"Testing set:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Standardize the features using the same scaler for consistency\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [3.5v]\n",
    "\n",
    "**Learn a MLP regressor with 2 hidden layers of size 10, rectifier linear unit activation\n",
    "on all nodes, and early stopping with 20% of training data set aside for validation. All\n",
    "remaining parameters (e.g., loss, batch size, regularization term, solver) should be set as\n",
    "default. Plot the distribution of the residues (in absolute value) using a histogram.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(10, 10), activation='relu', early_stopping=True, validation_fraction=0.2, random_state=0)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "residuals = []\n",
    "\n",
    "# Run the MLP regressor for 10 runs with different random seeds\n",
    "for random_state in range(1, 11):\n",
    "    mlp.set_params(random_state=random_state)  # Set the random seed\n",
    "    mlp.fit(X_train, y_train)  # Train the model\n",
    "    y_pred = mlp.predict(X_test)  # Make predictions\n",
    "    residual = np.abs(y_test - y_pred)  # Calculate absolute residuals\n",
    "    residuals.append(residual)\n",
    "\n",
    "# Plot the distribution of residuals using a histogram\n",
    "plt.hist(np.concatenate(residuals), bins=30, color='blue', alpha=0.7, edgecolor='black')\n",
    "plt.title(\"Distribution of Residuals\")\n",
    "plt.xlabel(\"Residual (Absolute Value)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) [1.5v]\n",
    "\n",
    "**Since we are in the presence of a integer regression task, a recommended trick is to\n",
    "round and bound estimates. Assess the impact of these operations on the MAE of the MLP\n",
    "learnt in previous question.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [256, 320]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/miguel/Univ/third-year/Apre/proj-APRE/src/hm3/HW3.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/miguel/Univ/third-year/Apre/proj-APRE/src/hm3/HW3.ipynb#Y166sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m rounded_predictions \u001b[39m=\u001b[39m [\u001b[39mround\u001b[39m(pred) \u001b[39mfor\u001b[39;00m pred \u001b[39min\u001b[39;00m y_pred]\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/miguel/Univ/third-year/Apre/proj-APRE/src/hm3/HW3.ipynb#Y166sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Calculate the MAE of the rounded predictions\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/miguel/Univ/third-year/Apre/proj-APRE/src/hm3/HW3.ipynb#Y166sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m mae \u001b[39m=\u001b[39m mean_absolute_error(y_val, rounded_predictions)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/miguel/Univ/third-year/Apre/proj-APRE/src/hm3/HW3.ipynb#Y166sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMAE of rounded predictions: \u001b[39m\u001b[39m{\u001b[39;00mmae\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/miguel/Univ/third-year/Apre/proj-APRE/src/hm3/HW3.ipynb#Y166sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Calculate the MAE of the unrounded predictions\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_regression.py:204\u001b[0m, in \u001b[0;36mmean_absolute_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m@validate_params\u001b[39m(\n\u001b[1;32m    141\u001b[0m     {\n\u001b[1;32m    142\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39marray-like\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, multioutput\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39muniform_average\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m ):\n\u001b[1;32m    152\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \n\u001b[1;32m    154\u001b[0m \u001b[39m    Read more in the :ref:`User Guide <mean_absolute_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39m    0.85...\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[39m=\u001b[39m _check_reg_targets(\n\u001b[1;32m    205\u001b[0m         y_true, y_pred, multioutput\n\u001b[1;32m    206\u001b[0m     )\n\u001b[1;32m    207\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    208\u001b[0m     output_errors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage(np\u001b[39m.\u001b[39mabs(y_pred \u001b[39m-\u001b[39m y_true), weights\u001b[39m=\u001b[39msample_weight, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_regression.py:99\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_reg_targets\u001b[39m(y_true, y_pred, multioutput, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     66\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39m        correct keyword.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[1;32m    100\u001b[0m     y_true \u001b[39m=\u001b[39m check_array(y_true, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    101\u001b[0m     y_pred \u001b[39m=\u001b[39m check_array(y_pred, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    405\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    408\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    410\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [256, 320]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Round the predicted values to the nearest integer\n",
    "rounded_predictions = [round(pred) for pred in y_pred]\n",
    "\n",
    "# Calculate the MAE of the rounded predictions\n",
    "mae = mean_absolute_error(y_val, rounded_predictions)\n",
    "\n",
    "print(f'MAE of rounded predictions: {mae}')\n",
    "\n",
    "# Calculate the MAE of the unrounded predictions\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "print(f'MAE of unrounded predictions: {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMMENT:\n",
    "\n",
    "The MAE of the rounded predictions is lower than the original MAE. This indicates that rounding the model's predictions to the nearest integer has improved the model's performance in terms of MAE. In this specific integer regression task, rounding the predictions appears to be a beneficial strategy, as it has reduced the absolute prediction errors, making the model's predictions closer to the actual integer wine quality scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [1.5v]\n",
    "\n",
    "**Similarly assess the impact on RMSE from replacing early stopping by a well-defined\n",
    "number of iterations in {20,50,100,200} (where one iteration corresponds to a batch).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for 20 iterations: 1.86178838665857\n",
      "RMSE for 50 iterations: 1.1908670767993699\n",
      "RMSE for 100 iterations: 0.9290572667642152\n",
      "RMSE for 200 iterations: 0.7143541567557457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def train_mlp_with_iterations(num_iterations):\n",
    "    rmse_sum = 0\n",
    "    for random_i in range(1,11):\n",
    "        \n",
    "        mlp = MLPRegressor(max_iter=num_iterations, random_state=random_i)\n",
    "        mlp.fit(X_train, y_train)\n",
    "    \n",
    "        y_pred = mlp.predict(X_test)\n",
    "        rmse_sum += np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    return rmse_sum/10\n",
    "\n",
    "iteration_values = [20, 50, 100, 200]\n",
    "\n",
    "rmse_values = []\n",
    "\n",
    "for num_iterations in iteration_values:\n",
    "    rmse = train_mlp_with_iterations(num_iterations)\n",
    "    rmse_values.append(rmse)\n",
    "\n",
    "# Print the RMSE values for different numbers of iterations\n",
    "for i, num_iterations in enumerate(iteration_values):\n",
    "    print(f'RMSE for {num_iterations} iterations: {rmse_values[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) [1.5v]\n",
    "\n",
    "**Critically comment the results obtained in previous question, hypothesizing at least\n",
    "one reason why early stopping favors and/or worsens performance.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, as the number of training iterations increases, the RMSE decreases, which is a positive sign. Lower RMSE values suggest that the model is improving its ability to predict the target variable more accurately. It's important to monitor the RMSE during training to assess the model's convergence and to determine when it reaches a point of diminishing returns or starts to overfit the data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
